"""GitHub Action audit module using Gemini CLI or local LLM for security analysis."""

import json
import os
import subprocess
import sys
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

import requests

from geminicli.constants import (
    DEFAULT_GEMINI_MODEL,
    DEFAULT_LOCAL_LLM_MODEL,
    DEFAULT_OLLAMA_BASE_URL,
    DEFAULT_LMSTUDIO_BASE_URL,
    EXIT_CONFIGURATION_ERROR,
    EXIT_GENERAL_ERROR,
    EXIT_SUCCESS,
    SUBPROCESS_TIMEOUT,
)
from geminicli.findings_filter import FindingsFilter
from geminicli.json_parser import parse_json_with_fallbacks
from geminicli.logger import get_logger
from geminicli.prompts import get_security_audit_prompt

logger = get_logger(__name__)


class ConfigurationError(Exception):
    """Raised when configuration is invalid or missing."""


class GitHubActionClient:
    """GitHub API interface for fetching PR data."""

    def __init__(self, token: str):
        self.token = token
        self.session = requests.Session()
        self.session.headers.update({
            "Authorization": f"Bearer {token}",
            "Accept": "application/vnd.github.v3+json",
            "X-GitHub-Api-Version": "2022-11-28",
        })

    def get_pr_data(self, repo: str, pr_number: int) -> Dict[str, Any]:
        """Fetch pull request metadata."""
        url = f"https://api.github.com/repos/{repo}/pulls/{pr_number}"
        resp = self.session.get(url, timeout=30)
        resp.raise_for_status()
        return resp.json()

    def get_pr_diff(self, repo: str, pr_number: int) -> str:
        """Fetch the unified diff for a pull request."""
        url = f"https://api.github.com/repos/{repo}/pulls/{pr_number}"
        headers = {"Accept": "application/vnd.github.v3.diff"}
        resp = self.session.get(url, headers=headers, timeout=60)
        resp.raise_for_status()
        return resp.text

    def get_pr_files(self, repo: str, pr_number: int) -> List[Dict[str, Any]]:
        """Fetch list of files changed in a pull request."""
        url = f"https://api.github.com/repos/{repo}/pulls/{pr_number}/files"
        resp = self.session.get(url, timeout=30)
        resp.raise_for_status()
        return resp.json()

    def is_generated_file(self, filename: str, patch: str = "") -> bool:
        """Check if a file is auto-generated."""
        generated_markers = [
            "@generated",
            "Code generated by",
            "DO NOT EDIT",
            "protoc-gen-go",
            "THIS FILE IS AUTO-GENERATED",
        ]
        if patch:
            for marker in generated_markers:
                if marker in patch:
                    return True
        generated_extensions = [".pb.go", "_pb2.py", ".generated.ts", ".g.cs"]
        return any(filename.endswith(ext) for ext in generated_extensions)


class SimpleGeminiRunner:
    """Gemini CLI runner for GitHub Actions security audits."""

    def __init__(self, timeout_minutes: Optional[int] = None, model: Optional[str] = None):
        """Initialize Gemini runner.

        Args:
            timeout_minutes: Timeout for Gemini execution (defaults to SUBPROCESS_TIMEOUT / 60)
            model: Gemini model to use
        """
        if timeout_minutes is not None:
            self.timeout_seconds = timeout_minutes * 60
        else:
            self.timeout_seconds = SUBPROCESS_TIMEOUT

        self.model = model or os.environ.get("GEMINI_MODEL") or DEFAULT_GEMINI_MODEL

    def run_security_audit(
        self, repo_dir: Path, prompt: str
    ) -> Tuple[bool, str, Dict[str, Any]]:
        """Run Gemini CLI security audit.

        Args:
            repo_dir: Path to repository directory
            prompt: Security audit prompt

        Returns:
            Tuple of (success, error_message, parsed_results)
        """
        if not repo_dir.exists():
            return False, f"Repository directory does not exist: {repo_dir}", {}

        prompt_size = len(prompt.encode("utf-8"))
        if prompt_size > 1024 * 1024:
            logger.warning(f"Large prompt size: {prompt_size / 1024 / 1024:.2f}MB")

        try:
            # Gemini CLI headless mode: pass prompt via stdin using -p flag
            # --output-format json gives structured output
            # --yolo auto-approves tool calls (needed for non-interactive CI use)
            cmd = [
                "gemini",
                "--output-format", "json",
                "--model", self.model,
                "--yolo",
                "-p", prompt,
            ]

            NUM_RETRIES = 3
            for attempt in range(NUM_RETRIES):
                logger.info(
                    f"Gemini CLI security audit attempt {attempt + 1}/{NUM_RETRIES}"
                )
                result = subprocess.run(
                    cmd,
                    cwd=repo_dir,
                    capture_output=True,
                    text=True,
                    timeout=self.timeout_seconds,
                    env={**os.environ, "GEMINI_API_KEY": os.environ.get("GEMINI_API_KEY", "")},
                )

                if result.returncode != 0:
                    if attempt < NUM_RETRIES - 1:
                        logger.warning(
                            f"Gemini CLI failed (attempt {attempt + 1}), retrying..."
                        )
                        time.sleep(5 * (attempt + 1))
                        continue
                    else:
                        error_details = (
                            f"Gemini CLI execution failed with return code "
                            f"{result.returncode}\n"
                            f"Stderr: {result.stderr}\n"
                            f"Stdout: {result.stdout[:500]}..."
                        )
                        return False, error_details, {}

                # Gemini CLI --output-format json returns:
                # { "response": "...", "stats": {...} }
                # Extract the response field and parse embedded JSON findings
                success, parsed_outer = parse_json_with_fallbacks(
                    result.stdout, "Gemini CLI stdout"
                )

                if not success:
                    if attempt < NUM_RETRIES - 1:
                        logger.warning("Failed to parse Gemini output, retrying...")
                        time.sleep(5)
                        continue
                    return False, "Failed to parse Gemini CLI output", {}

                # Extract security findings from the response text
                response_text = ""
                if isinstance(parsed_outer, dict):
                    response_text = parsed_outer.get("response", "")
                elif isinstance(parsed_outer, str):
                    response_text = parsed_outer

                if not response_text:
                    # Try parsing stdout directly as the findings JSON
                    response_text = result.stdout

                parsed_results = self._extract_security_findings(response_text)
                return True, "", parsed_results

            return False, "Unexpected error in retry logic", {}

        except subprocess.TimeoutExpired:
            return (
                False,
                f"Gemini CLI execution timed out after "
                f"{self.timeout_seconds // 60} minutes",
                {},
            )
        except Exception as e:
            return False, f"Gemini CLI execution error: {str(e)}", {}

    def _extract_security_findings(self, response_text: str) -> Dict[str, Any]:
        """Extract security findings JSON from Gemini's response text."""
        if not response_text:
            return self._empty_findings()

        success, result_json = parse_json_with_fallbacks(
            response_text, "Gemini response text"
        )

        if success and isinstance(result_json, dict) and "findings" in result_json:
            return result_json

        # If no findings key, return empty
        return self._empty_findings()

    def _empty_findings(self) -> Dict[str, Any]:
        return {
            "findings": [],
            "analysis_summary": {
                "files_reviewed": 0,
                "high_severity": 0,
                "medium_severity": 0,
                "low_severity": 0,
                "review_completed": False,
            },
        }

    def validate_gemini_available(self) -> Tuple[bool, str]:
        """Validate that Gemini CLI is installed and API key is set."""
        # Check API key
        api_key = os.environ.get("GEMINI_API_KEY", "")
        if not api_key:
            return False, "GEMINI_API_KEY environment variable is not set"

        # Check Gemini CLI is installed
        try:
            result = subprocess.run(
                ["gemini", "--version"],
                capture_output=True,
                text=True,
                timeout=10,
            )
            if result.returncode == 0:
                logger.info(f"Gemini CLI available: {result.stdout.strip()}")
                return True, ""
            else:
                error_msg = f"Gemini CLI returned exit code {result.returncode}"
                if result.stderr:
                    error_msg += f". Stderr: {result.stderr}"
                return False, error_msg
        except subprocess.TimeoutExpired:
            return False, "Gemini CLI command timed out"
        except FileNotFoundError:
            return False, "Gemini CLI is not installed or not in PATH. Run: npm install -g @google/gemini-cli"
        except Exception as e:
            return False, f"Failed to check Gemini CLI: {str(e)}"


class LocalLLMRunner:
    """Runner that calls a local LLM via OpenAI-compatible REST API.

    Supports Ollama (default) and LM Studio.
    No external CLI needed — uses only the `requests` package.

    Environment variables:
        LLM_PROVIDER      : 'ollama' (default) | 'lmstudio'
        LOCAL_LLM_MODEL   : model name, e.g. 'llama3.2', 'qwen2.5-coder'
        LOCAL_LLM_BASE_URL: override the base URL (e.g. remote Ollama server)
        LLM_TIMEOUT_MINUTES: analysis timeout in minutes (default: 20)
    """

    def __init__(
        self,
        timeout_minutes: Optional[int] = None,
        model: Optional[str] = None,
        base_url: Optional[str] = None,
        provider: Optional[str] = None,
    ):
        self.provider = (
            provider
            or os.environ.get("LLM_PROVIDER", "ollama").lower()
        )
        self.model = (
            model
            or os.environ.get("LOCAL_LLM_MODEL")
            or DEFAULT_LOCAL_LLM_MODEL
        )
        self.timeout_seconds = (timeout_minutes or 20) * 60

        # Resolve base URL
        if base_url:
            self.base_url = base_url.rstrip("/")
        elif os.environ.get("LOCAL_LLM_BASE_URL"):
            self.base_url = os.environ["LOCAL_LLM_BASE_URL"].rstrip("/")
        elif self.provider == "lmstudio":
            self.base_url = DEFAULT_LMSTUDIO_BASE_URL
        else:
            self.base_url = DEFAULT_OLLAMA_BASE_URL

        # OpenAI-compatible chat completions endpoint
        self.chat_url = f"{self.base_url}/v1/chat/completions"

    def run_security_audit(
        self, repo_dir: Path, prompt: str
    ) -> Tuple[bool, str, Dict[str, Any]]:
        """Run security audit via local LLM REST API.

        Args:
            repo_dir: Path to repository directory (unused but kept for interface parity)
            prompt: Security audit prompt

        Returns:
            Tuple of (success, error_message, parsed_results)
        """
        NUM_RETRIES = 3
        # response_format requires guided decoding support.
        # Disable with LOCAL_LLM_JSON_MODE=false if your model doesn't support it.
        use_json_mode = os.environ.get("LOCAL_LLM_JSON_MODE", "true").lower() != "false"
        payload: Dict[str, Any] = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "stream": False,
        }
        if use_json_mode:
            payload["response_format"] = {"type": "json_object"}

        for attempt in range(NUM_RETRIES):
            logger.info(
                f"Local LLM ({self.provider}/{self.model}) audit attempt "
                f"{attempt + 1}/{NUM_RETRIES}"
            )
            try:
                resp = requests.post(
                    self.chat_url,
                    json=payload,
                    timeout=self.timeout_seconds,
                )
                resp.raise_for_status()
            except requests.exceptions.Timeout:
                if attempt < NUM_RETRIES - 1:
                    logger.warning("Request timed out, retrying...")
                    time.sleep(5 * (attempt + 1))
                    continue
                return (
                    False,
                    f"Local LLM timed out after {self.timeout_seconds // 60} minutes",
                    {},
                )
            except requests.exceptions.ConnectionError as e:
                return (
                    False,
                    f"Cannot connect to {self.base_url}. "
                    f"Is {self.provider} running? Error: {e}",
                    {},
                )
            except requests.exceptions.HTTPError as e:
                if attempt < NUM_RETRIES - 1:
                    logger.warning(f"HTTP error {resp.status_code}, retrying...")
                    time.sleep(5 * (attempt + 1))
                    continue
                return False, f"HTTP error from local LLM: {e}", {}

            data = resp.json()
            response_text = (
                data.get("choices", [{}])[0]
                .get("message", {})
                .get("content", "")
            )

            if not response_text:
                if attempt < NUM_RETRIES - 1:
                    logger.warning("Empty response, retrying...")
                    time.sleep(5)
                    continue
                return False, "Empty response from local LLM", {}

            parsed_results = self._extract_security_findings(response_text)
            return True, "", parsed_results

        return False, "Unexpected error in retry logic", {}

    def _extract_security_findings(self, response_text: str) -> Dict[str, Any]:
        """Extract security findings JSON from response text."""
        success, result_json = parse_json_with_fallbacks(
            response_text, "Local LLM response"
        )
        if success and isinstance(result_json, dict) and "findings" in result_json:
            return result_json
        return {
            "findings": [],
            "analysis_summary": {
                "files_reviewed": 0,
                "high_severity": 0,
                "medium_severity": 0,
                "low_severity": 0,
                "review_completed": False,
            },
        }

    def validate_gemini_available(self) -> Tuple[bool, str]:
        """Validate that the local LLM server is reachable and the model exists."""
        # Check server health
        health_url = f"{self.base_url}/api/tags" if self.provider == "ollama" else f"{self.base_url}/v1/models"
        try:
            resp = requests.get(health_url, timeout=5)
            resp.raise_for_status()
        except requests.exceptions.ConnectionError:
            return (
                False,
                f"Cannot connect to {self.provider} at {self.base_url}. "
                f"Start it with: {'ollama serve' if self.provider == 'ollama' else 'LM Studio → Local Server'}",
            )
        except Exception as e:
            return False, f"Health check failed: {e}"

        # For Ollama: verify the model is pulled
        if self.provider == "ollama":
            models_data = resp.json()
            available = [m.get("name", "").split(":")[0] for m in models_data.get("models", [])]
            model_base = self.model.split(":")[0]
            if available and model_base not in available:
                return (
                    False,
                    f"Model '{self.model}' not found in Ollama. "
                    f"Run: ollama pull {self.model}\n"
                    f"Available models: {', '.join(available)}",
                )

        logger.info(f"Local LLM ready: {self.provider}/{self.model} @ {self.base_url}")
        return True, ""


def get_environment_config() -> Tuple[str, int]:
    """Get and validate required environment variables.

    Returns:
        Tuple of (repo_name, pr_number)

    Raises:
        ConfigurationError: If required variables are missing
    """
    repo = os.environ.get("GITHUB_REPOSITORY", "")
    if not repo:
        raise ConfigurationError(
            "GITHUB_REPOSITORY environment variable is required (e.g. owner/repo)"
        )

    pr_number_str = os.environ.get("PR_NUMBER", "")
    if not pr_number_str:
        raise ConfigurationError("PR_NUMBER environment variable is required")

    try:
        pr_number = int(pr_number_str)
    except ValueError:
        raise ConfigurationError(
            f"PR_NUMBER must be an integer, got: {pr_number_str}"
        )

    return repo, pr_number


def initialize_clients() -> Tuple[GitHubActionClient, Union[SimpleGeminiRunner, LocalLLMRunner]]:
    """Initialize GitHub client and the appropriate LLM runner.

    Runner selection via LLM_PROVIDER environment variable:
      - 'gemini'   (default) → SimpleGeminiRunner  (requires GEMINI_API_KEY)
      - 'ollama'             → LocalLLMRunner       (requires running Ollama server)
      - 'lmstudio'           → LocalLLMRunner       (requires running LM Studio)

    Returns:
        Tuple of (GitHubActionClient, runner)

    Raises:
        ConfigurationError: If GITHUB_TOKEN is missing
    """
    github_token = os.environ.get("GITHUB_TOKEN", "")
    if not github_token:
        raise ConfigurationError(
            "GITHUB_TOKEN environment variable is required"
        )

    timeout_str = os.environ.get("GEMINICLI_TIMEOUT_MINUTES", "20")
    try:
        timeout_minutes = int(timeout_str)
    except ValueError:
        timeout_minutes = 20

    github_client = GitHubActionClient(token=github_token)

    provider = os.environ.get("LLM_PROVIDER", "gemini").lower()
    if provider in ("ollama", "lmstudio"):
        runner: Union[SimpleGeminiRunner, LocalLLMRunner] = LocalLLMRunner(
            timeout_minutes=timeout_minutes,
            provider=provider,
        )
        logger.info(f"Using local LLM runner: {provider}")
    else:
        runner = SimpleGeminiRunner(
            timeout_minutes=timeout_minutes,
            model=os.environ.get("GEMINI_MODEL") or None,
        )
        logger.info("Using Gemini CLI runner")

    return github_client, runner


def initialize_findings_filter(
    custom_filtering_instructions: Optional[str] = None,
) -> FindingsFilter:
    """Initialize the findings filter.

    Args:
        custom_filtering_instructions: Unused, kept for API compatibility

    Returns:
        Configured FindingsFilter instance
    """
    return FindingsFilter(use_hard_exclusions=True)


def apply_findings_filter(
    findings_filter: FindingsFilter,
    original_findings: List[Dict[str, Any]],
    pr_context: Dict[str, Any],
    github_client: GitHubActionClient,
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], Dict[str, Any]]:
    """Apply findings filter and directory exclusions.

    Args:
        findings_filter: Configured filter instance
        original_findings: Raw findings from Gemini
        pr_context: PR metadata for context
        github_client: GitHub client (unused currently but kept for parity)

    Returns:
        Tuple of (kept_findings, excluded_findings, analysis_summary)
    """
    exclude_dirs_str = os.environ.get("EXCLUDE_DIRECTORIES", "")
    exclude_dirs = [d.strip() for d in exclude_dirs_str.split(",") if d.strip()]

    # First filter by excluded directories
    dir_excluded = []
    remaining = []
    for finding in original_findings:
        file_path = finding.get("file", "")
        if any(file_path.startswith(d) or f"/{d}/" in file_path for d in exclude_dirs):
            finding["exclusion_reason"] = "excluded_directory"
            dir_excluded.append(finding)
        else:
            remaining.append(finding)

    # Apply findings filter
    success, filter_results, stats = findings_filter.filter_findings(
        remaining, pr_context=pr_context
    )

    if not success:
        logger.warning("Findings filter failed, returning unfiltered results")
        return remaining, dir_excluded, {}

    kept = filter_results.get("filtered_findings", remaining)
    filter_excluded = filter_results.get("excluded_findings", [])

    all_excluded = dir_excluded + filter_excluded
    analysis_summary = {
        "total_original": len(original_findings),
        "dir_excluded": len(dir_excluded),
        "filter_excluded": len(filter_excluded),
        "kept": len(kept),
        "filter_stats": {
            "hard_excluded": stats.hard_excluded,
            "gemini_excluded": getattr(stats, "gemini_excluded", 0),
        },
    }

    return kept, all_excluded, analysis_summary


def main() -> None:
    """Main execution function for GitHub Action."""
    try:
        # Get environment configuration
        try:
            repo_name, pr_number = get_environment_config()
        except ConfigurationError as e:
            print(json.dumps({"error": str(e)}))
            sys.exit(EXIT_CONFIGURATION_ERROR)

        # Load custom filtering instructions
        custom_filtering_instructions = None
        filtering_file = os.environ.get("FALSE_POSITIVE_FILTERING_INSTRUCTIONS", "")
        if filtering_file and Path(filtering_file).exists():
            try:
                with open(filtering_file, "r", encoding="utf-8") as f:
                    custom_filtering_instructions = f.read()
                logger.info(f"Loaded custom filtering instructions from {filtering_file}")
            except Exception as e:
                logger.warning(f"Failed to read filtering instructions: {e}")

        # Load custom security scan instructions
        custom_scan_instructions = None
        scan_file = os.environ.get("CUSTOM_SECURITY_SCAN_INSTRUCTIONS", "")
        if scan_file and Path(scan_file).exists():
            try:
                with open(scan_file, "r", encoding="utf-8") as f:
                    custom_scan_instructions = f.read()
                logger.info(f"Loaded custom scan instructions from {scan_file}")
            except Exception as e:
                logger.warning(f"Failed to read scan instructions: {e}")

        # Initialize components
        try:
            github_client, gemini_runner = initialize_clients()
        except ConfigurationError as e:
            print(json.dumps({"error": str(e)}))
            sys.exit(EXIT_CONFIGURATION_ERROR)

        # Initialize findings filter
        findings_filter = initialize_findings_filter(custom_filtering_instructions)

        # Validate Gemini CLI is available
        gemini_ok, gemini_error = gemini_runner.validate_gemini_available()
        if not gemini_ok:
            print(json.dumps({"error": f"Gemini CLI not available: {gemini_error}"}))
            sys.exit(EXIT_GENERAL_ERROR)

        # Fetch PR data
        try:
            pr_data = github_client.get_pr_data(repo_name, pr_number)
            pr_diff = github_client.get_pr_diff(repo_name, pr_number)
        except Exception as e:
            print(json.dumps({"error": f"Failed to fetch PR data: {str(e)}"}))
            sys.exit(EXIT_GENERAL_ERROR)

        # Generate security audit prompt
        prompt = get_security_audit_prompt(
            pr_data,
            pr_diff,
            custom_scan_instructions=custom_scan_instructions,
        )

        # Run Gemini CLI security audit
        repo_path = os.environ.get("REPO_PATH")
        repo_dir = Path(repo_path) if repo_path else Path.cwd()

        success, error_msg, results = gemini_runner.run_security_audit(repo_dir, prompt)

        # If prompt is too long, retry without diff
        if not success and "too long" in error_msg.lower():
            logger.info("Prompt too long, retrying without diff")
            prompt_no_diff = get_security_audit_prompt(
                pr_data,
                pr_diff,
                include_diff=False,
                custom_scan_instructions=custom_scan_instructions,
            )
            success, error_msg, results = gemini_runner.run_security_audit(
                repo_dir, prompt_no_diff
            )

        if not success:
            print(json.dumps({"error": f"Security audit failed: {error_msg}"}))
            sys.exit(EXIT_GENERAL_ERROR)

        # Filter findings
        original_findings = results.get("findings", [])
        pr_context = {
            "repo_name": repo_name,
            "pr_number": pr_number,
            "title": pr_data.get("title", ""),
            "description": pr_data.get("body", ""),
        }

        kept_findings, excluded_findings, analysis_summary = apply_findings_filter(
            findings_filter, original_findings, pr_context, github_client
        )

        # Build output
        output = {
            "pr_number": pr_number,
            "repo": repo_name,
            "findings": kept_findings,
            "analysis_summary": results.get("analysis_summary", {}),
            "filtering_summary": {
                "total_original_findings": len(original_findings),
                "excluded_findings": len(excluded_findings),
                "kept_findings": len(kept_findings),
                "filter_analysis": analysis_summary,
                "excluded_findings_details": excluded_findings,
            },
        }

        print(json.dumps(output, indent=2))

        # Exit with appropriate code
        high_count = len(
            [f for f in kept_findings if f.get("severity", "").upper() == "HIGH"]
        )
        sys.exit(EXIT_GENERAL_ERROR if high_count > 0 else EXIT_SUCCESS)

    except Exception as e:
        print(json.dumps({"error": f"Unexpected error: {str(e)}"}))
        sys.exit(EXIT_CONFIGURATION_ERROR)


if __name__ == "__main__":
    main()
